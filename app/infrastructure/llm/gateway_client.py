import httpx
from typing import Optional
import logging
from app.schemas.diagnosis_dto import DiagnosisResult
from app.config.settings import settings

logger = logging.getLogger(__name__)

class LLMGatewayClient:
    """LLM Gatewayì™€ í†µì‹ í•˜ëŠ” í´ë¼ì´ì–¸íŠ¸"""

    def __init__(
        self,
        gateway_url: str,
        provider: str = "noop",
        model: str = "gpt-4o-mini",
        api_key: Optional[str] = None,
        timeout: int = 30
    ):
        """
        Args:
            gateway_url: LLM Gateway ì„œë²„ URL (ì˜ˆ: http://localhost:3030)
            provider: LLM ì œê³µì (noop, openai, anthropic ë“±)
            model: ëª¨ë¸ëª…
            api_key: API í‚¤ (optional, Gatewayì—ì„œ ê´€ë¦¬í•  ìˆ˜ë„ ìˆìŒ)
            timeout: íƒ€ì„ì•„ì›ƒ(ì´ˆ)
        """
        self.gateway_url = gateway_url.rstrip("/")
        self.provider=provider.lower()
        self.model = model
        self.api_key = api_key
        self.timeout = timeout

        if self.provider == 'noop':
            logger.info("ğŸ§ª LLM Client: NoOp ëª¨ë“œ (í…ŒìŠ¤íŠ¸ìš©, ê³¼ê¸ˆ ì—†ìŒ)")
        else:
            logger.info(f"ğŸš€ LLM Client: {provider} / {model}")


    def generate_feedback(
        self,
        diagnosis: DiagnosisResult,
        user_id: str,
        club: str,
        tone: str = "professional",
        language: str = "ko"
    ) -> str:
        """
        ì§„ë‹¨ ê²°ê³¼ ê¸°ë°˜ AI í”¼ë“œë°± ìƒì„±

        Args:
            diagnosis: ì§„ë‹¨ ê²°ê³¼
            user_id: ì‚¬ìš©ì ID
            club: í´ëŸ½ ì¢…ë¥˜
            tone: ì–´ì¡° (professional, friendly, coach)
            language: ì–¸ì–´ (ko, en)

        Returns:
            AIê°€ ìƒì„±í•œ í”¼ë“œë°± í…ìŠ¤íŠ¸
        """
        # noop ëª¨ë“œ- mock ì‘ë‹µ ì¦‰ì‹œ return
        if self.provider == "noop":
            logger.info("Noop ëª¨ë“œ: Mock ì‘ë‹µ ë°˜í™˜ (ê³¼ê¸ˆ ì—†ìŒ)")
            return self._generate_mock_feedback(diagnosis)

        # System prompt
        system_prompt = self._build_system_prompt(tone, language)

        # User prompt (ì§„ë‹¨ ìš”ì•½)
        user_prompt = self._build_user_prompt(diagnosis, club)

        # LLM Gateway í˜¸ì¶œ
        try:
            response = httpx.post(
                f"{self.gateway_url}/api/chat",
                json={
                    "provider": self.provider,
                    "model": self.model,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "user_id": user_id,
                    "temperature": 0.7,
                    "max_tokens": 500
                },
                headers={"X-API-Key": self.api_key} if self.api_key else {},
                timeout=self.timeout
            )
            response.raise_for_status()

            result = response.json()
            return result.get("content", "í”¼ë“œë°± ìƒì„± ì‹¤íŒ¨")

        except httpx.HTTPError as e:
            # Fallback: ë£° ê¸°ë°˜ í”¼ë“œë°±
            return self._fallback_feedback(diagnosis)

    def _build_system_prompt(self, tone: str, language: str) -> str:
        """System prompt ìƒì„±"""
        tone_map = {
            "professional": "ì „ë¬¸ì ì´ê³  ê°ê´€ì ì¸",
            "friendly": "ì¹œê·¼í•˜ê³  ê²©ë ¤í•˜ëŠ”",
            "coach": "ì½”ì¹˜ì²˜ëŸ¼ êµ¬ì²´ì ì¸ ì‹¤ì²œ ë°©ì•ˆì„ ì œì‹œí•˜ëŠ”"
        }

        tone_desc = tone_map.get(tone, "ì „ë¬¸ì ì´ê³  ê°ê´€ì ì¸")

        if language == "ko":
            return f"""ë‹¹ì‹ ì€ í”„ë¡œ ê³¨í”„ ì½”ì¹˜ì…ë‹ˆë‹¤.
ìŠ¤ìœ™ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ {tone_desc} ì–´ì¡°ë¡œ í”¼ë“œë°±ì„ ì œê³µí•˜ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
- 5-8ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±
- ê°€ì¥ ì¤‘ìš”í•œ ë¬¸ì œì  2-3ê°œë§Œ ì–¸ê¸‰
- êµ¬ì²´ì ì¸ ê°œì„  ë°©ë²• ì œì‹œ
- ê¸ì •ì ì¸ ë¶€ë¶„ë„ í•¨ê»˜ ì–¸ê¸‰
"""
        else:  # English
            return f"""You are a professional golf coach.
Provide feedback based on swing analysis results in a {tone_desc} tone.

Requirements:
- Keep it concise (5-8 sentences)
- Mention only 2-3 most important issues
- Provide specific improvement methods
- Include positive aspects as well
"""

    def _build_user_prompt(self, diagnosis: DiagnosisResult, club: str) -> str:
        """User prompt (ì§„ë‹¨ ìš”ì•½) ìƒì„±"""
        lines = [f"í´ëŸ½: {club}", f"ì „ì²´ ì ìˆ˜: {diagnosis.overall_score:.1f}/100", ""]

        for d in diagnosis.diagnoses:
            lines.append(f"[{d.phase}] ì ìˆ˜: {d.score:.1f}")
            if d.issues:
                lines.append(f"  ë¬¸ì œì : {', '.join(d.issues[:2])}")  # ìµœëŒ€ 2ê°œë§Œ

        return "\n".join(lines)

    def _fallback_feedback(self, diagnosis: DiagnosisResult) -> str:
        """LLM ì‹¤íŒ¨ ì‹œ ë£° ê¸°ë°˜ í”¼ë“œë°±"""
        feedback_lines = [f"ìŠ¤ìœ™ ì „ì²´ ì ìˆ˜: {diagnosis.overall_score:.1f}/100"]

        # ê°€ì¥ ë‚®ì€ ì ìˆ˜ í˜ì´ì¦ˆ ì°¾ê¸°
        worst_phase = min(diagnosis.diagnoses, key=lambda d: d.score)

        feedback_lines.append(f"\nê°€ì¥ ê°œì„ ì´ í•„ìš”í•œ ë‹¨ê³„: {worst_phase.phase} (ì ìˆ˜: {worst_phase.score:.1f})")

        if worst_phase.issues:
            feedback_lines.append("\nì£¼ìš” ë¬¸ì œì :")
            for issue in worst_phase.issues[:2]:
                feedback_lines.append(f"- {issue}")

        if worst_phase.suggestions:
            feedback_lines.append("\nê°œì„  ì œì•ˆ:")
            for suggestion in worst_phase.suggestions[:2]:
                feedback_lines.append(f"- {suggestion}")

        return "\n".join(feedback_lines)

    def _generate_mock_feedback(self, diagnosis: DiagnosisResult) -> str:
        """
        Noop ëª¨ë“œìš© Mock í”¼ë“œë°± ìƒì„±

        Args:
            diagnosis: ì§„ë‹¨ ê²°ê³¼

        Returns:
            Mock í”¼ë“œë°± í…ìŠ¤íŠ¸
        """
        feedback_lines = [
            "[í…ŒìŠ¤íŠ¸ ëª¨ë“œ - NoOp LLM]",
            "",
            f"âœ… ìŠ¤ìœ™ ë¶„ì„ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            f"ì „ì²´ ì ìˆ˜: {diagnosis.overall_score:.1f}/100",
            ""
        ]

        # ê°€ì¥ ë‚®ì€ ì ìˆ˜ í˜ì´ì¦ˆ ì°¾ê¸°
        if diagnosis.diagnoses:
            worst_phase = min(diagnosis.diagnoses, key=lambda d: d.score)
            feedback_lines.append(f"ê°œì„ ì´ í•„ìš”í•œ ë‹¨ê³„: {worst_phase.phase} (ì ìˆ˜: {worst_phase.score:.1f})")

            if worst_phase.issues:
                feedback_lines.append("")
                feedback_lines.append("ì£¼ìš” ë¬¸ì œì :")
                for issue in worst_phase.issues[:2]:
                    feedback_lines.append(f"- {issue}")

        feedback_lines.extend([
            "",
            "í˜„ì¬ëŠ” í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ, ì‹¤ì œ LLM APIë¥¼ í˜¸ì¶œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
            "ê³¼ê¸ˆì´ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
            "",
            "ì‹¤ì œ AI í”¼ë“œë°±ì„ ë°›ìœ¼ë ¤ë©´:",
            "1. Swagger UIì—ì„œ llm_providerë¥¼ 'openai'ë¡œ ë³€ê²½",
            "2. llm_modelì„ 'gpt-4o-mini' ë“±ìœ¼ë¡œ ì„¤ì •",
            "3. Execute ì‹¤í–‰"
        ])
        
        return "\n".join(feedback_lines)

    def generate_text(
        self,
        user_prompt: str,
        system_prompt: str = "",
        model: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 500,
        timeout: float = 30.0
    ) -> str:
        """
        ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ìƒì„± (ë³´ê³ ì„œ ë“±)
        
        Args:
            user_prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            model: ëª¨ë¸ëª… (ì—†ìœ¼ë©´ self.model ì‚¬ìš©)
            temperature: ì˜¨ë„
            max_tokens: ìµœëŒ€ í† í°
            timeout: íƒ€ì„ì•„ì›ƒ
            
        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        # noop ëª¨ë“œ: mock ì‘ë‹µ
        if self.provider == "noop":
            logger.info("Noop ëª¨ë“œ: Mock í…ìŠ¤íŠ¸ ë°˜í™˜")
            return self._generate_mock_text(user_prompt)
        
        # LLM Gateway í˜¸ì¶œ
        try:
            response = httpx.post(
                f"{self.gateway_url}/api/chat",
                json={
                    "provider": self.provider,
                    "model": model or self.model,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "temperature": temperature,
                    "max_tokens": max_tokens
                },
                headers={"X-API-Key": self.api_key} if self.api_key else {},
                timeout=timeout
            )
            response.raise_for_status()
            result = response.json()
            return result.get("content", "í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨")
        
        except httpx.HTTPError as e:
            logger.error(f"LLM Gateway í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return self._generate_fallback_text(user_prompt)
    
    def _generate_mock_text(self, user_prompt: str) -> str:
        """Noop ëª¨ë“œìš© Mock í…ìŠ¤íŠ¸"""
        return f"""[í…ŒìŠ¤íŠ¸ ëª¨ë“œ - NoOp LLM]

ì´ê²ƒì€ ì‹¤ì œ LLM APIë¥¼ í˜¸ì¶œí•˜ì§€ ì•Šê³  ë°˜í™˜ë˜ëŠ” Mock ì‘ë‹µì…ë‹ˆë‹¤.
ê³¼ê¸ˆì´ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ì…ë ¥ëœ í”„ë¡¬í”„íŠ¸:
{user_prompt[:200]}...

ì‹¤ì œ AI ì‘ë‹µì„ ë°›ìœ¼ë ¤ë©´ llm_providerë¥¼ 'openai' ë˜ëŠ” 'anthropic'ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.
"""
    
    def _generate_fallback_text(self, user_prompt: str) -> str:
        """LLM ì‹¤íŒ¨ ì‹œ Fallback í…ìŠ¤íŠ¸"""
        return f"""[LLM ì„œë¹„ìŠ¤ ì¼ì‹œì  ì˜¤ë¥˜]

ì£„ì†¡í•©ë‹ˆë‹¤. AI ì„œë¹„ìŠ¤ ì—°ê²°ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.
ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.

ì…ë ¥ëœ í”„ë¡¬í”„íŠ¸: {user_prompt[:100]}...
"""


def get_llm_client(
    provider: Optional[str] = None,
    model: Optional[str] = None,
    gateway_url: Optional[str] = None,
    api_key: Optional[str] = None
) -> LLMGatewayClient:
    """
    LLM í´ë¼ì´ì–¸íŠ¸ íŒ©í† ë¦¬ í•¨ìˆ˜
    
    Args:
        provider: LLM ì œê³µì (ì—†ìœ¼ë©´ settingsì—ì„œ ê°€ì ¸ì˜´)
        model: ëª¨ë¸ëª… (ì—†ìœ¼ë©´ settingsì—ì„œ ê°€ì ¸ì˜´)
        gateway_url: Gateway URL (ì—†ìœ¼ë©´ settingsì—ì„œ ê°€ì ¸ì˜´)
        api_key: API í‚¤ (ì—†ìœ¼ë©´ settingsì—ì„œ ê°€ì ¸ì˜´)
        
    Returns:
        LLMGatewayClient ì¸ìŠ¤í„´ìŠ¤
    """
    return LLMGatewayClient(
        gateway_url=gateway_url or settings.LLM_GATEWAY_URL,
        provider=provider or settings.LLM_DEFAULT_PROVIDER,
        model=model or settings.LLM_DEFAULT_MODEL,
        api_key=api_key or settings.OPENAI_API_KEY,
        timeout=30
    )